Automated Trading System on various market environments in FinRL

Abstract

This project explores the application of model-free reinforcement learning (RL) algorithms within the FinRL framework to develop an automated trading system. Five RL agents were trained on twelve years of daily stock data and tested over a two-year period with a starting budget of one million dollars. Additionally, sentiment analysis was integrated into the trading strategy to evaluate its impact on investment decisions. The performance of these agents was compared against traditional trading strategies such as Mean Variance Optimization (MVO) and the Dow Jones Industrial Average (DJIA).

Introduction

The growing complexity of financial markets and the vast amounts of data generated by them necessitate the development of advanced trading systems that can learn and adapt in dynamic environments. This project aims to leverage model-free RL to handle such complexities without assuming any specific market model. The objective is to evaluate the effectiveness of these 5 RL algorithms: A2C, DDPG, PPO, SAC, and TD3, in navigating the stock market using both historical price data, some feature engineered attributes with sentiments from some source (twitter, now X). The integration of sentiment data aims to mimic the decision-making process of human traders who consider not only past performance but also the current market sentiment. Give 5 RL agents 20 years worth of daily stock data to learn.

Then, give $1 Million to those RL agents and see how much would they make at 4 years (rolling prediction over 2 years on a daily basis)

In progress: Give those agents sentiments from twitter (now X), to calibrate itâ€™s position and invest accordingly.

Finally, compare between agents with and without sentiment from twitter with some baseline trading techniques like moving variance optimization and Dow Jones Average.

Methodology

**Data Acquisition and Preprocessing**

Following the feedback from the project presentation, I expanded the stock options from 5 to 10, selecting them from various sectors. Here is the breakdown:

**Technology:**

- AAPL (Apple Inc.) - Consumer Electronics
- MSFT (Microsoft Corporation) - Software and Cloud Solutions
- NVDA (NVIDIA Corporation) - Semiconductors and AI Technology

**Consumer Discretionary:**

- AMZN (Amazon.com Inc.) - E-commerce and Cloud Computing
- DIS (Walt Disney Co.) - Entertainment and Media
- CCL (Carnival Corporation) - Leisure and Hospitality (Cruises)
- SBUX (Starbucks Corporation) - Specialty Eateries

**Industrials:**

- BA (Boeing) - Aerospace and Defense

**Consumer Staples:**

- WMT (Walmart Inc.) - Retail - General Merchandise

**Financials:**

- BAC (Bank of America Corp.) - Banking and Financial Services

Although the selection remains skewed towards technology, I faced challenges in finding 'emotions' for various types of companies on any platform. Therefore, I selected well-discussed stocks for my project. Since Twitter itself started in 2006 and the emotions were sourced from StockEmotions by Jean Lee et al., and my training timeline was from 2000 to 2020, I had to impute a lot of missing data. The sentiment classes included bullish (positive), bearish (negative), and emotion classes such as ambiguous, amusement, anger, anxiety, belief, confusion, depression, disgust, excitement, optimism, panic, surprise. To address missing data, I utilized a 2-layer Bidirectional LSTM to leverage the sequential properties of the stock data. Stock data was obtained from Yahoo Finance using the FinRL library, which provides tools for financial data processing. The preprocessing steps included feature engineering where technical indicators like MACD, RSI, and Bollinger Bands were calculated to serve as inputs to the RL models.

**Indicators:**

- **MACD (Moving Average Convergence Divergence):** A trend-following momentum indicator that shows the relationship between two exponential moving averages of a security's price. It consists of the MACD line and the signal line, which is a moving average of the MACD line.
- **Bollinger Bands:** A volatility-based technical analysis tool consisting of a middle band (usually a simple moving average) flanked by two standard deviation bands above and below to calculate the next upper and lower bands.
- **RSI (Relative Strength Index) 30:** A momentum oscillator that measures the speed and change of price movements, oscillating between 0 and 100. It is typically used to identify overbought or oversold conditions in a security.
- **CCI (Commodity Channel Index) 30:** An indicator that can be used to identify a new trend or warn of extreme conditions by measuring the difference between a security's price change and its average price change.
- **DX (Directional Movement Index) 30:** Used to determine the strength of a price trend by comparing the difference between two recent highs and the difference between two recent lows, then smoothing the result.
- **Close 30 SMA (Simple Moving Average):** The 30-day simple moving average of the closing prices, a widely used indicator that smooths out price data by creating a constantly updated average price.
- **Close 60 SMA (Simple Moving Average):** Similar to the 30-day SMA but with a longer timeframe, this is the 60-day simple moving average of the closing prices.
- **VIX:** The CBOE Volatility Index, commonly referred to as the "fear index," represents the market's expectations for volatility over the coming 30 days.
- **Turbulence:** Measures the variation in returns of different assets in a portfolio at a given time, indicative of a volatile market.

**RL Model Implementation Configuration Parameters:**

- Hmax (maximum episode length): 100
- Initial amount: $1,000,000
- Initial stocks: None
- Buy and Sell cost: 0.01
- State space: 101
- Action space: 5
- Reward scaling: 10-4

**Implemented RL Algorithms:**

**A2C (Advantage Actor-Critic):**

**Type:** Model-Free, On-Policy

A2C is a synchronous, deterministic variant of the Asynchronous Advantage Actor-Critic (A3C), utilizing an actor-critic architecture. The actor proposes actions based on the current policy, while the critic evaluates these actions by comparing the predicted value of the state-action pair against the current state's estimated value. This setup helps refine both the policy and value function, enhancing the stability and simplicity of the algorithm.

**DDPG (Deep Deterministic Policy Gradient):**

**Type:** Model-Free, Off-Policy

DDPG is suitable for continuous action spaces and employs a deterministic policy, which consistently outputs the same action for a given state. This feature simplifies learning and optimization. The algorithm uses deep neural networks to represent both the policy and value functions, incorporates experience replay, and utilizes target networks to ensure stable convergence. DDPG's robustness makes it ideal for complex control tasks like robotic manipulation and navigation.

**PPO (Proximal Policy Optimization):**

**Type:** Model-Free, On-Policy

PPO optimizes the policy function directly using a clipped surrogate objective, which limits the size of policy updates to avoid significant deviations from the current policy. This constraint enhances the algorithm's stability and sample efficiency, making PPO a popular choice for both discrete and continuous action spaces due to its ease of implementation and tolerance to hyperparameter settings.

**TD3 (Twin Delayed Deep Deterministic policy gradient):**

**Type:** Model-Free, Off-Policy

An enhancement over DDPG, TD3 reduces overestimation bias and variance in Q-value estimates by employing twin Q-networks and taking the minimum Q-value between them. It also features delayed updates for both the policy and target Q-networks, and target policy smoothing, which together improve training stability and prevent policy collapse. TD3 is particularly effective in continuous action spaces and demonstrates improved performance and sample efficiency.

**SAC (Soft Actor-Critic):**

**Type:** Model-Free, Off-Policy

SAC integrates the maximum entropy framework with the actor-critic architecture, focusing on maximizing both the expected return and the entropy of the policy. This dual objective leads to more exploratory and robust policies. SAC uses a soft value function to learn a soft Q-function and a stochastic policy, which supports efficient exploration in high-dimensional spaces. It is well-regarded for its state-of-the-art performance in complex RL tasks.

**Understanding the Baselines:**

**Mean Variance Optimization (MVO):**

MVO is a foundational portfolio optimization technique that maximizes the expected return for a given risk level, assessed as the variance of portfolio returns. It constructs an efficient frontier of optimal portfolios, offering a spectrum of risk-return trade-offs, and typically selects the portfolio with the highest Sharpe ratio.

**Dow Jones Industrial Average (DJIA):**

The DJIA is an index representing the performance of 30 large, publicly-traded companies in the U.S. It serves as a benchmark to evaluate the overall market performance and to compare investment strategies. In this study, the DJIA provides a conventional benchmark against which the performance of the RL models is assessed.

Results

The RL agents demonstrated varying degrees of success, with each showing strengths and weaknesses depending on market conditions. The addition of sentiment analysis generally improved the performance of the agents, suggesting that real-time market sentiment is valuable in automated trading systems. The models were benchmarked against MVO and DJIA, where some RL models outperformed these traditional strategies, especially in volatile market conditions.

Conclusion

The study confirmed that model-free RL could be effectively applied to automated trading systems, adapting to new market environments and integrating external sentiment data. While not all models consistently outperformed traditional benchmarks, the results indicate substantial potential for RL in financial applications. Future work could explore the integration of more diverse data sources and the development of hybrid models that combine the strengths of different RL algorithms.

Appendices

Graph of the portfolio over 4 years of trading by our 5 RL models. Here dotted lines are models without sentiments and emotions.

![](![Aspose Words 45559aa3-3259-492c-a70b-a1eb15ee21f9 001](https://github.com/git4sudo/fin_rl_vs_llm/assets/46088769/d2da0c0a-6c5f-4afb-b017-cba4aea8f1eb)
)Table of comparison of Max, min and final monetary values obtained from models with and without sentiments and emotions.



|Model / Values|Least Amount ever held|Most Amount ever held|Amount at the end|
| - | - | - | - |
|Mean Variation Optimization|722,855|2,920,148|2,890,714|
|DJIA|644,014|1,306,257|1,306,257|
|A2C|715,441|1,434,573|1,360,408|
|A2C without sentiments|576,758|1,349,856|1,344,732|
|DDPG|696,038|1,543,716|1,543,716|
|DDPG without sentiments|554,862|1,445,747|1,433,522|
|PPO|**803,332**|**2,984,424**|**2,942,377**|
|PPO without sentiments|671,632|1,250,572|1,159,688|
|SAC|474,544|1,050,981|907,285|
|SAC without sentiments|657,506|1,291,987|983,601|
|TD3|768,308|2,338,554|2,327,253|
|TD3 without sentiments|598,117|1,518,512|1,505,299|

